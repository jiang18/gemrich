#' Estimate Genetic Effect Enrichments for Annotation Categories Using Maximum Likelihood
#'
#' This function performs maximum likelihood estimation (MLE) for the probability of causal variants being in each functional annotation category. 
#' The output includes estimated probabilities, standard errors, enrichment fold values, 
#' and profile-likelihood confidence intervals of probabilities (available only for binary functional annotations).
#' 
#' @param bfmap A `data.frame` or `data.table` containing fine-mapping summary statistics generated by BFMAP.
#'        For forward selection: required columns are signal, SNPindex, Pval, normedProb, and SNPname.
#'        For SSS: columns should be locus ID, variant names, and model probabilities.
#' @param snpinfo A `data.frame` or `data.table` containing SNP functional annotation data, which can be generated using `map_snp_annotation()`.
#' @param cat_prop A `data.frame` or `data.table` containing the proportion of SNPs in each annotation category. 
#'        This can be generated using either `calc_snp_category_prop()` or `calc_category_coverage()`.
#' @param annot A character string specifying the functional annotation column in snpinfo (default: "multi_cat").
#' @param pvalue_threshold A numeric threshold for filtering fine-mapped signals based on p-values (default: 5e-5). 
#'        Only signals with lead SNP p-values below this threshold are included in the analysis.
#'        Only used for forward selection format.
#' @param input_type Character string specifying the BFMAP input format: "forward_selection" (default) or "sss".
#' @param model_prob_cutoff A numeric threshold for cumulative model probability cutoff (default: 0.8).
#'        Only used for SSS format. Models are included until cumulative probability exceeds this threshold.
#' 
#' @return A list containing:
#'   \item{prob_mle}{Maximum likelihood estimates of the probabilities of causal variants being in annotation categories}
#'   \item{prob_cov_matrix}{Covariance matrix of the probability estimates}
#'   \item{loglik}{Log-likelihood value at the maximum}
#'   \item{enrichment_mle}{Maximum likelihood estimates of genetic effect enrichments}
#' 
#' @examples
#' data("dairy_example")
#' # Forward selection (default)
#' mle_result <- estimate_category_enrichment(
#'   dairy_example$bfmap,
#'   dairy_example$snp2annot,
#'   dairy_example$cat_prop,
#'   pvalue_threshold = 5e-5
#' )
#' 
#' # SSS
#' # mle_result_sss <- estimate_category_enrichment(
#' #   sss_data,
#' #   snp2annot,
#' #   cat_prop,
#' #   input_type = "sss",
#' #   model_prob_cutoff = 0.9
#' # )
#' 
#' @export
#' 
estimate_category_enrichment <- function(bfmap, snpinfo, cat_prop, annot = "multi_cat", pvalue_threshold = 5e-5, input_type = "forward_selection", model_prob_cutoff = 0.8) {
  
  # Handle SSS format
  if (input_type == "sss") {
    return(estimate_category_enrichment_sss(bfmap, snpinfo, cat_prop, annot, model_prob_cutoff))
  }
  
  # Forward selection
  # Validate inputs
  pvalue_threshold = as.numeric(pvalue_threshold)
  if(is.na(pvalue_threshold)) {
	stop("'pvalue_threshold' must be numeric.\n", call.=FALSE)
  }
  # Load data
  kept_signals <- bfmap$signal[bfmap$SNPindex == 0 & bfmap$Pval < pvalue_threshold]
  if(length(kept_signals) != length(unique(kept_signals))) {
    stop("Multiple signals have identical signal index.", call.=FALSE)
  }
  bfmap <- bfmap[bfmap$signal %in% kept_signals, ]
  setDT(bfmap)
  nloci = length(kept_signals)

  if (!(annot %in% colnames(snpinfo))) stop(paste(annot, "is not one of the column names in 'snpinfo'"))

  cat_prop[[1]] = factor(cat_prop[[1]], levels=cat_prop[[1]])
  cat_names = levels(cat_prop[[1]])

  snpinfo <- copy(as.data.table(snpinfo))
  snp_colname = colnames(snpinfo)[1]
  snpinfo = copy(snpinfo)[, .SD, .SDcols = c(snp_colname, annot)]
  snpinfo = snpinfo[snpinfo[[snp_colname]] %in% unique(bfmap$SNPname), ]
  snpinfo[is.na(snpinfo[[annot]]),2] = "remaining"
  if(! all(unique(snpinfo[[annot]]) %in% cat_names) ) {
    stop("Some categories in 'snpinfo' are missing in 'cat_prop'.\n", call.=FALSE)
  }
  snpinfo[[annot]] = factor(snpinfo[[annot]], levels=cat_names)
  cat_cnt = table(snpinfo[[annot]])
  
  levels(snpinfo[[annot]])= c(1:length(cat_names))
  var2cat = new.env(hash=TRUE)
  cats = apply(snpinfo, 1, function(x) var2cat[[x[1]]] = x[annot])
  bfmap[, cat_idx := as.integer(unlist(sapply(SNPname, function(x) var2cat[[x]])))]

  cat("Number of unique model SNPs in each category:")
  print(cat_cnt)
  if(any(cat_cnt < 50)) {
    cat("\nWarning: Some categories involve too few signal SNPs, which may cause ML estimation problems.\n")
  }
  flush.console()

  if(any(cat_prop[[2]]<=0)) {
    stop("Category proportions in cat_prop must be positive.\n", call.=FALSE)
  }
  freq = cat_prop[[2]]
  if(sum(freq) != 1) {
    cat("\nSum of proportions in cat_prop is not equal to 1. Scaled to 1.\n")
    freq = freq / sum(freq)
  }

  cat(paste("\nCompleted reading all data files.\n",
      "The ML estimation uses ", nloci, " loci and ", nrow(bfmap)," variants.\n",sep=""))
  flush.console()

  logLL = function(par) {
    par <- c(par, 1-sum(par))   # sum(par) = 1
    if(!any(par<0)) {
      # li = rep(0, nloci)
      # for(i in c(1:nloci)) {
      #   d = bfmap[bfmap$signal==kept_signals[i],]
      #   pp = par[d$cat_idx]
      #   qp = freq[d$cat_idx]
      #   li[i] = sum(d[[pcol]]*pp/qp)
      # }
      ret = bfmap[, .(li = sum(normedProb * par[cat_idx] / freq[cat_idx])), by = signal][, sum(log(li))]
      return(ret)
    } else return(-100000)
  }

  par = freq[1:(length(freq)-1)]
  npars = length(par)
  result = NA

  eps = 1e-12
  if(npars == 1) {
    result <- optim(par, logLL, method="Brent",
            hessian=T,
            lower=rep(eps, npars),
            upper=rep(1-eps, npars),
            control=list(fnscale=-1))
  } else {
    # First try optim
    result <- try(optim(par, logLL, method="L-BFGS-B",
              hessian=T,
              lower=rep(eps, npars),
              upper=rep(1-eps, npars),
              control=list(fnscale=-1)), silent=TRUE)
    # If optim fails, try constrOptim
    if(inherits(result, "try-error") || result$convergence != 0 || any(result$par < 1e-3)) {
      cat("Initial optimization failed. Trying constrOptim...\n")
      flush.console()

      ui = rbind(diag(npars), rep(-1, npars))
      ci = c(rep(eps, npars), -1)
      result_constr <- constrOptim(theta=par, f=logLL, grad=NULL,
                  ui=ui, ci=ci,
                  control=list(fnscale=-1))
      # Add Hessian to result_constr before assigning to result
      result_constr$hessian <- try(optimHess(result_constr$par, logLL))
      result <- result_constr
      cat("constrOptim completed.\n")
      flush.console()
    }
  }
  cat("Completed MLE.\n")

  covar_mle = solve(-result$hessian)
  cat("Calculated SE with Hessian.\n")
  flush.console()

  # Profile likelihood method for SE
  profile_se <- function(mle, delta = seq(-0.1, 0.1, length=41)) {
    # Calculate profile likelihood around MLE
    profile_points <- mle + delta
    profile_points <- profile_points[profile_points>0 & profile_points<1]
    profile_values <- sapply(profile_points, logLL)

    # Maximum likelihood at MLE
    max_ll <- max(profile_values)

    # Find points where log-likelihood drops by 1.92
    # (approximately 95% CI region)
    cutoff <- max_ll - 1.92
    valid_points <- profile_points[profile_values >= cutoff]

    # SE estimate from the range
    se <- diff(range(valid_points))/3.84

    # Return full information for diagnostics
    return(list(
      se = se,
      profile_points = profile_points,
      profile_values = profile_values,
      cutoff = cutoff,
      ci = range(valid_points)
    ))
  }

  mle <- result$par
  prob_mle = copy(cat_prop)
  setDT(prob_mle)
  prob_mle[, paste0("V", 3:7) := NA_real_]
  prob_mle[[3]]=c(mle, 1-sum(mle))

  prob_cov_matrix <- rbind(
    cbind(covar_mle, -covar_mle %*% rep(1, nrow(covar_mle))),
    c(-covar_mle %*% rep(1, nrow(covar_mle)), sum(covar_mle))
  )
  prob_mle[[4]] = sqrt(diag(prob_cov_matrix))
  if(npars == 1) {
    profileLL_result <- profile_se(mle)
    cat("Completed profile likelihood (used only for binary annotations).\n")
    prob_mle[[5]] = rep(profileLL_result$se, 2)
    prob_mle[1, (6:7) := as.list(profileLL_result$ci)]
    prob_mle[2, (6:7) := as.list(rev(1-profileLL_result$ci))]
  }
  colnames(prob_mle)[2:7] = c("q", "p_MLE", "Hessian_SE", "profile_SE", "profile_95lower", "profile_95upper")
  colnames(prob_cov_matrix) = levels(cat_prop[[1]])

  enrichment_mle = prob_mle[,1:4]
  colnames(enrichment_mle) =c("category","enrichment","SE","p")
  enrichment_mle[,2] = prob_mle$p_MLE / prob_mle$q
  enrichment_mle[,3] = prob_mle$Hessian_SE / prob_mle$q
  enrichment_mle[,4] = pnorm( (enrichment_mle[[2]]-1)/enrichment_mle[[3]], lower.tail = F)

  output <- list()
  output[["prob_mle"]] = prob_mle
  output[["prob_cov_matrix"]] = prob_cov_matrix
  output[["loglik"]] = result$value
  output[["convergence"]] = result$convergence
  output[["counts"]] = cat_cnt
  output[["enrichment_mle"]] = enrichment_mle
  return(output)
}

# SSS function 
estimate_category_enrichment_sss <- function(bfmap, snpinfo, cat_prop, annot = "multi_cat", model_prob_cutoff = 0.8) {
  
  # Input validation
  model_prob_cutoff <- as.numeric(model_prob_cutoff)
  if(is.na(model_prob_cutoff)) {
    stop("Probability-cutoff must be numeric.\n", call.=FALSE)
  }
  
  setDT(bfmap)
  setDT(snpinfo)
  cat_prop <- cat_prop[,1:2]
  
  # Annotation processing - optimized
  if(!(annot %in% colnames(snpinfo))) {
    stop(paste("Annotation [", annot, "] is not available in 'snpinfo'.\n"), call.=FALSE)
  }
  
  # Filter annotation to only variants present in bfmap
  if(colnames(bfmap)[1] != "locus") {
    stop("The first column must be locus in 'bfmap'.\n", call.=FALSE)
  }
  pcol <- ncol(bfmap)
  bfmap_variants <- unique(unlist(bfmap[, 2:(pcol-2), with=FALSE], use.names=FALSE))
  bfmap_variants <- bfmap_variants[!is.na(bfmap_variants)]

  snp_colname = colnames(snpinfo)[1]
  snpinfo = copy(snpinfo)[, .SD, .SDcols = c(snp_colname, annot)]
  snpinfo <- snpinfo[get(snp_colname) %in% bfmap_variants]
  set(snpinfo, which(is.na(snpinfo[[annot]])), annot, "remaining")

  if(!all(bfmap_variants %in% snpinfo[[snp_colname]])) {
    missing_variants <- bfmap_variants[!(bfmap_variants %in% snpinfo[[snp_colname]])]
    stop("Missing variants in 'snpinfo': ", 
         paste(head(missing_variants, 5), collapse = ", "),
         if(length(missing_variants) > 5) " ..." else "",
         call. = FALSE)
  }
  
  # Use categories from cat_prop consistently  
  cat_prop[[1]] = factor(cat_prop[[1]], levels=cat_prop[[1]])
  cat_names = levels(cat_prop[[1]])
  n_cats <- length(cat_names)
  
  # Validate that annotation categories match cat_prop
  if(!all(unique(snpinfo[[annot]]) %in% cat_names)) {
    stop("Categories in annotation data are not all present in cat_prop.\n", call.=FALSE)
  }
  
  # Create environment-based lookup (Document 6 approach)
  snpinfo[[annot]] = factor(snpinfo[[annot]], levels=cat_names)
  levels(snpinfo[[annot]]) = c(1:length(cat_names))
  
  var2cat = new.env(hash=TRUE)
  cats = apply(snpinfo, 1, function(x) var2cat[[x[1]]] = x[annot])
  
  # Unified frequency processing - consistent with forward selection
  freq <- cat_prop[[2]]
  if(any(freq <= 0)) {
    stop("Category probabilities in cat_prop must be positive.\n", call.=FALSE)
  }
  if(sum(freq) != 1) {
    cat("\nSum of proportions in cat_prop is not equal to 1. Scaled to 1.\n")
    freq <- freq / sum(freq)
  }
  
  log_freq <- log(freq)
  
  # Data preprocessing - create optimized data structures
  locus_levels <- unique(bfmap$locus)
  nloci <- length(locus_levels)
  prob_col <- names(bfmap)[pcol]
  
  # Pre-process ALL data into optimized matrix format
  cat("Preprocessing data for fast computation...\n")
  flush.console()
  
  # Create sparse representation of models
  locus_info <- vector("list", nloci)
  
  for(i in seq_len(nloci)) {
    locus_subset <- bfmap[locus == locus_levels[i]]
    # Sort by probability column in descending order
    setorderv(locus_subset, prob_col, order = -1)
    
    pcum <- cumsum(locus_subset[[pcol]])
    cutoff_idx <- which(pcum > model_prob_cutoff)[1]
    if(is.na(cutoff_idx)) cutoff_idx <- nrow(locus_subset)
    
    locus_subset <- locus_subset[seq_len(cutoff_idx)]
    
    # Extract variants and convert to category indices
    var_cols <- 2:(pcol-2)
    model_data <- as.matrix(locus_subset[, var_cols, with=FALSE])
    probs <- locus_subset[[pcol]]
    
    # Pre-compute category indices for each model using environment lookup
    model_cats <- vector("list", nrow(model_data))
    model_log_contributions <- numeric(nrow(model_data))
    
    for(m in seq_len(nrow(model_data))) {
      variants <- model_data[m, ]
      variants <- variants[!is.na(variants)]
      
      if(length(variants) > 0) {
        # Environment-based lookup 
        cats <- mget(as.character(variants), envir = var2cat, ifnotfound = list(NA))
        model_cats[[m]] <- as.integer(unlist(cats))
        # Pre-compute the constant part (log probability)
        model_log_contributions[m] <- log(probs[m])
      } else {
        model_cats[[m]] <- integer(0)
        model_log_contributions[m] <- log(probs[m])
      }
    }
    
    locus_info[[i]] <- list(
      model_cats = model_cats,
      log_probs = model_log_contributions,
      n_models = length(model_cats)
    )
  }
  
  total_models <- sum(sapply(locus_info, function(x) x$n_models))
  cat(paste("Preprocessing complete. Using", nloci, "loci and", total_models, "models.\n"))
  flush.console()
  
  par <- freq[seq_len(n_cats-1)]
  npars <- length(par)
  eps <- 1e-10
  
  cat("Starting optimization...\n")
  flush.console()
  
  result <- NULL
  
  if(npars == 1) {
    result <- optim(par, logLL_rcpp, method="Brent",
            hessian=TRUE, 
            lower=eps, upper=1-eps,
            control=list(fnscale=-1), locus_info=locus_info, log_freq=log_freq)
  } else {
    result <- try(optim(par, logLL_rcpp, method="L-BFGS-B",
              hessian=TRUE,
              lower=rep(eps, npars), upper=rep(1-eps, npars),
              control=list(fnscale=-1), locus_info=locus_info, log_freq=log_freq), silent=TRUE)
    # If optim fails, try constrOptim
    if(inherits(result, "try-error") || result$convergence != 0 || any(result$par < 1e-3)) {
      cat("Initial optimization failed. Trying constrOptim...\n")
      flush.console()

      ui = rbind(diag(npars), rep(-1, npars))
      ci = c(rep(eps, npars), -1)
      result_constr <- constrOptim(theta=par, f=logLL_rcpp, grad=NULL,
                  ui=ui, ci=ci,
                  control=list(fnscale=-1), locus_info=locus_info, log_freq=log_freq)
      # Add Hessian to result_constr before assigning to result
      result_constr$hessian <- try(optimHess(result_constr$par, logLL_rcpp, locus_info=locus_info, log_freq=log_freq))
      result <- result_constr
      cat("constrOptim completed.\n")
      flush.console()
    }
  }
  
  cat("Completed MLE.\n")
  flush.console()
  
  covar_mle <- try(solve(-result$hessian), silent=TRUE)
  if(inherits(covar_mle, "try-error")) {
    covar_mle <- diag(rep(0.5^2, npars))
  }
  
  cat("Calculated SE with Hessian.\n")
  flush.console()
  
  # Profile likelihood for binary case only (single dimension)
  profile_se <- function(mle, delta = seq(-0.1, 0.1, length=41)) {
    # Calculate profile likelihood around MLE
    profile_points <- mle + delta
    profile_points <- profile_points[profile_points>0 & profile_points<1]
    profile_values <- sapply(profile_points, logLL_rcpp, locus_info=locus_info, log_freq=log_freq)

    # Maximum likelihood at MLE
    max_ll <- max(profile_values)

    # Find points where log-likelihood drops by 1.92
    # (approximately 95% CI region)
    cutoff <- max_ll - 1.92
    valid_points <- profile_points[profile_values >= cutoff]

    # Handle case where valid_points may be empty
    if(length(valid_points) > 40) {
      # Fallback: use a large SE to signal estimation problems
      se <- 0.5  # Large value indicates unreliable estimate
      ci <- c(0, 1)  # Widest possible CI reflecting complete uncertainty
      warning("Profile likelihood confidence interval estimation failed - using fallback values")
    } else {
      # SE estimate from the range
      se <- diff(range(valid_points))/3.84
      ci <- range(valid_points)
    }

    # Return only se and ci (cleaner than full diagnostics)
    return(list(
      se = se,
      ci = ci
    ))
  }
  
  # Prepare results with unified structure
  mle <- result$par
  prob_mle <- copy(cat_prop)
  setDT(prob_mle)
  prob_mle[, paste0("V", 3:7) := NA_real_]
  prob_mle[[3]] <- c(mle, 1-sum(mle))
  
  prob_cov_matrix <- rbind(
    cbind(covar_mle, -covar_mle %*% rep(1, nrow(covar_mle))),
    c(-covar_mle %*% rep(1, nrow(covar_mle)), sum(covar_mle))
  )
  
  prob_mle[[4]] <- sqrt(pmax(0, diag(prob_cov_matrix)))
  
  if(npars == 1) {
    profileLL_result <- profile_se(mle[1])
    cat("Completed profile likelihood (used only for binary annotations).\n")
    prob_mle[[5]] <- rep(profileLL_result$se, 2)
    prob_mle[1, (6:7) := as.list(profileLL_result$ci)]
    prob_mle[2, (6:7) := as.list(rev(1-profileLL_result$ci))]
  }
  
  colnames(prob_mle)[2:7] <- c("q", "p_MLE", "Hessian_SE", "profile_SE", "profile_95lower", "profile_95upper")
  colnames(prob_cov_matrix) <- cat_names
  
  cat_cnt <- table(snpinfo[[annot]])
  
  enrichment_mle <- prob_mle[,1:4]
  colnames(enrichment_mle) <- c("category", "enrichment", "SE", "p")
  enrichment_mle[,2] <- prob_mle$p_MLE / prob_mle$q
  enrichment_mle[,3] <- prob_mle$Hessian_SE / prob_mle$q
  enrichment_mle[,4] <- pnorm((enrichment_mle[[2]]-1)/enrichment_mle[[3]], lower.tail = FALSE)
  
  return(list(
    prob_mle = prob_mle,
    prob_cov_matrix = prob_cov_matrix,
    loglik = result$value,
    convergence = result$convergence,
    counts = cat_cnt,
    enrichment_mle = enrichment_mle
  ))
}
